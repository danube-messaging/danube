# Danube cluster name
cluster_name: "MY_CLUSTER"

# Broker Services Configuration
broker:
  # Hostname or IP address for all broker services
  host: "0.0.0.0"
  ports:
    # Client connections (producers/consumers)
    client: 6650
    # Admin API
    admin: 50051
    # Prometheus metrics exporter
    prometheus: 9040

# Metadata Store Configuration (etcd)
meta_store:
  host: "127.0.0.1"
  port: 2379

# Namespaces to be created on boot
bootstrap_namespaces:
  - "default"

# Allow producers to auto-create topics when missing
auto_create_topics: true

# Security Configuration
auth:
  mode: none # Options: none, tls, tlswithjwt
  # tls:
  #   cert_file: "./cert/server-cert.pem"
  #   key_file: "./cert/server-key.pem"
  #   ca_file: "./cert/ca-cert.pem"
  #   verify_client: false
  # jwt:
  #   secret_key: "your-secret-key"
  #   issuer: "danube-auth"
  #   expiration_time: 3600 # in seconds

# Load Manager Configuration (Automated Proactive Rebalancing)
load_manager:
  # Topic Assignment Strategy: How to assign NEW topics to brokers
  # Options:
  #   - fair: Simple topic count only (predictable, testing-friendly)
  #           Formula: topic_count only
  #           Aliases: round_robin
  #
  #   - balanced: Multi-factor scoring (RECOMMENDED for production)
  #               Formula: (weighted_topic_load × 0.3) + (CPU × 0.35) + (Memory × 0.35)
  #               Weighted topic load = count×0.2 + throughput×0.3 + connections×0.3 + backlog×0.2
  #               Uses per-topic metrics: message_rate, byte_rate, producer/consumer counts
  #               Aliases: least_loaded
  #
  #   - weighted_load: Adaptive smart algorithm (advanced)
  #                    Automatically detects bottleneck resources and prioritizes them
  #                    If CPU >70% utilized → prioritize CPU in scoring
  #                    If throughput >70% → prioritize bandwidth
  #                    Adapts to workload patterns dynamically
  #                    Aliases: weighted_random
  assignment_strategy: "balanced"

  # LoadReport Publishing Interval: How often brokers publish load reports (seconds)
  # - Lower values (5-10s): Faster response, more etcd traffic, better for testing
  # - Higher values (30-60s): Less overhead, suitable for stable production clusters
  # Default: 30 seconds
  load_report_interval_seconds: 30

  # Automated Rebalancing: Proactive optimization of EXISTING topic placement
  rebalancing:
    # Enable/disable automated rebalancing (starts disabled for safety)
    enabled: false

    # Aggressiveness level: Controls how aggressively to optimize cluster balance
    # Options:
    #   - conservative: Fewer moves, only when severely imbalanced (CV > 40%), more stable
    #   - balanced: Moderate optimization (CV > 30%), good for most production clusters
    #   - aggressive: Optimize harder (CV > 20%), more moves, responds faster to changes
    aggressiveness: "balanced"

    # Check frequency: How often to evaluate cluster balance (seconds)
    # Defaults by aggressiveness: conservative=600, balanced=300, aggressive=180
    check_interval_seconds: 300

    # Move limits: Prevent rebalancing storms
    # Rebalancing moves 1 topic per cycle (hardcoded for safety)
    # Max topic moves per hour (rate limiting across all cycles)
    # Defaults by aggressiveness: conservative=5, balanced=10, aggressive=20
    max_moves_per_hour: 10

    # Wait time between individual topic moves (seconds)
    # Defaults by aggressiveness: conservative=120, balanced=60, aggressive=30
    cooldown_seconds: 60

    # Cluster requirements: Minimum brokers needed to enable rebalancing
    # Rebalancing is skipped if cluster has fewer brokers than this
    min_brokers_for_rebalance: 2

    # Topic protection: Don't move topics younger than this (seconds)
    # Prevents moving topics that were just created or recently moved
    min_topic_age_seconds: 300

    # Topic blacklist: Topics matching these patterns will NEVER be rebalanced
    # Supports:
    #   - Exact topic names: /admin/critical-topic
    #   - Namespace wildcards: /system/* (all topics in /system namespace)
    # Examples: ["/system/*", "/admin/critical-topic", "/production/*"]
    blacklist_topics: []

# WAL and Cloud Storage Configuration
wal_cloud:
  wal:
    dir: "./danube-data/wal"
    file_name: "wal.log"
    cache_capacity: 1024
    # WAL file sync configuration, drive flush cadence and checkpoint frequency
    file_sync:
      interval_ms: 5000 # Default:Flush every 5s to reduce fsync pressure while keeping WAL checkpoints reasonably fresh
      max_batch_bytes: 10485760 # Default: 10 MiB writer batch
    # WAL file rotation configuration, balances file size and directory churn
    rotation:
      max_bytes: 536870912 # Default: 512 MiB WAL file size
      # max_hours: 24           # Optional time-based rotation (in hours)
    # WAL retention policy (time + size). Both applied each cycle; safety-checked against uploader progress.
    retention:
      time_minutes: 2880 # 48 hours (conservative production default)
      size_mb: 20480 # 20 GiB per topic
      check_interval_minutes: 5 # run every 5 minutes

  # Uploader, runs every interval_seconds, reads raw WAL frames, writes to configured cloud storage
  uploader:
    enabled: true
    interval_seconds: 30 # Default: Upload every 5 minutes
    root_prefix: "/danube-data"
    # Optional: maximum size per cloud object created in a single tick (in MiB). If unset, no cap.
    max_object_mb: 256

  # Cloud storage backend - Default: memory for development
  cloud:
    backend: "memory" # options: memory | fs | s3 | gcs
    root: "mem-prefix" # when backend: memory, this is a namespace prefix (ignored by some backends)

  # Metadata configuration - Default: ETCD
  metadata:
    etcd_endpoint: "127.0.0.1:2379"
    in_memory: false

# Broker policies, that can be overwritten by namespace / topic policies
policies:
  # Limits the maximum number of producers that can simultaneously publish messages to a specific topic.
  # Default is 0, unlimited.
  max_producers_per_topic: 0

  # Limits the maximum number of subscriptions that can be created on the topic.
  # Default is 0, unlimited.
  max_subscriptions_per_topic: 0

  # Limits the maximum number of consumers that can simultaneously consume messages from a specific topic.
  # Default is 0, unlimited.
  max_consumers_per_topic: 0

  # Limits the maximum number of consumers that can simultaneously use a single subscription on a topic.
  # Default is 0, unlimited.
  max_consumers_per_subscription: 0

  # Defines the Max publish rate (number of messages and/or bytes per second) for producers publishing to the topic.
  # Default is 0, unlimited.
  max_publish_rate: 0

  # Defines the dispatch rate for each subscription on the topic.
  # Default is 0, unlimited.
  max_subscription_dispatch_rate: 0

  # Limits the maximum size of a single message that can be published to the topic.
  # Default is 10 MB
  max_message_size: 10485760 # in bytes which means 10 MB

#
# --- Cloud backend examples ---
#
# Example: Local filesystem backend
#wal_cloud:
#  cloud:
#    backend: "fs"
#    root: "./object_root"     # directory where object data will be stored

# Example: Amazon S3 backend
#wal_cloud:
#  cloud:
#    backend: "s3"
#    root: "s3://my-bucket/prefix"  # bucket and optional prefix
#    region: "us-east-1"
#    endpoint: "https://s3.us-east-1.amazonaws.com"   # optional custom endpoint
#    access_key: "${AWS_ACCESS_KEY_ID}"               # or omit and rely on env/instance profile
#    secret_key: "${AWS_SECRET_ACCESS_KEY}"
#    profile: null                                      # optional AWS profile name
#    role_arn: null                                     # optional role to assume
#    session_token: null                                # optional session token
#    anonymous: false                                   # true to use anonymous access for public buckets

# Example: Google Cloud Storage backend
#wal_cloud:
#  cloud:
#    backend: "gcs"
#    root: "gcs://my-bucket/prefix"
#    project: "my-gcp-project"
#    credentials_json: null                             # inline JSON (string)
#    credentials_path: "/path/to/creds.json"          # or a path to credentials file

# Example: Azure Blob Storage backend
#wal_cloud:
#  cloud:
#    backend: "azblob"
#    root: "my-container/prefix"                       # container or container/prefix
#    endpoint: "https://<account>.blob.core.windows.net" # or Azurite: http://127.0.0.1:10000/devstoreaccount1
#    account_name: "<account_name>"
#    account_key: "<account_key>"
